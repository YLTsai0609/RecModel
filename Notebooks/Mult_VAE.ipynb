{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.sparse as sp\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torchsummary\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy\n",
    "import torch.autograd\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coo_to_sparse_tensor(coo_mat):\n",
    "    values = coo_mat.data\n",
    "    indices = np.vstack((coo_mat.row, coo_mat.col))\n",
    "\n",
    "    i = torch.LongTensor(indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo_mat.shape\n",
    "\n",
    "    return torch.sparse.FloatTensor(i, v, torch.Size(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = scipy.sparse.load_npz('../data/mat_bin_train.npz')\n",
    "eval_mat = scipy.sparse.load_npz('../data/mat_bin_validate.npz')\n",
    "\n",
    "train_mat_normalized = normalize(X=train_mat, norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_sum_dim_1(sparse_tensor, device):\n",
    "    x = torch.sparse.FloatTensor(\n",
    "            indices=torch.stack([\n",
    "                sparse_tensor._indices()[0],\n",
    "                torch.LongTensor(1).to(device).zero_().expand_as(sparse_tensor._indices()[0]),\n",
    "            ]),\n",
    "            values=sparse_tensor._values(),\n",
    "            size=[sparse_tensor.shape[0], 1]).to(device)\n",
    "\n",
    "    x.coalesce()\n",
    "    return x.to_dense().squeeze()\n",
    "\n",
    "def sparse_dense_mul(s, d):\n",
    "  i = s._indices()\n",
    "  v = s._values()\n",
    "  dv = d[i[0,:], i[1,:]]  # get values from relevant entries of dense matrix\n",
    "  return torch.sparse.FloatTensor(i, v * dv, s.size()).to_dense()\n",
    "\n",
    "def sparse_dense_mul_sparse_output(s, d):\n",
    "      i = s._indices()\n",
    "      v = s._values()\n",
    "      dv = d[i[0,:], i[1,:]]  # get values from relevant entries of dense matrix\n",
    "      return torch.sparse.FloatTensor(i, v * dv, s.size())def DenseLayers(in_channels, out_channels, batch_norm=False, dropout_rate=None, activation=False):\n",
    "        individual_layers = []\n",
    "            \n",
    "        if not dropout_rate is None:\n",
    "            individual_layers.append(nn.Dropout(p=dropout_rate))\n",
    "            \n",
    "        individual_layers.append(nn.Linear(in_channels, out_channels))\n",
    "\n",
    "        if activation is not None:\n",
    "            individual_layers.append(nn.Tanh())\n",
    "            \n",
    "        if batch_norm is not None:\n",
    "            individual_layers.append(nn.BatchNorm1d(num_features=out_channels))\n",
    "\n",
    "        return nn.Sequential(*individual_layers)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, k, dense_layers, batch_norm, activation, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.k  = k\n",
    "        self.additional_layer = len(dense_layers)\n",
    "        if self.additional_layer > 2:\n",
    "            all_layers = [DenseLayers(in_channels=in_channels, out_channels=out_channels, dropout_rate=dropout_rate,\n",
    "                                                          batch_norm=batch_norm, activation=activation) for (in_channels, out_channels) in zip(dense_layers[1:-1], dense_layers[2:])]\n",
    "            \n",
    "            all_layers[0:0] = [DenseLayers(in_channels=dense_layers[0], out_channels=dense_layers[1], batch_norm=batch_norm, dropout_rate=None, activation=activation)]\n",
    "            self.dense_network = nn.Sequential(*all_layers)\n",
    "        elif self.additional_layer > 1:\n",
    "            self.dense_network = DenseLayers(in_channels=dense_layers[0], out_channels=dense_layers[1], batch_norm=batch_norm, dropout_rate=None, activation=activation)\n",
    "        else:\n",
    "            pass\n",
    "        self.out_layer = nn.Linear(dense_layers[-1], 2 * k)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if self.additional_layer > 1:\n",
    "            X = self.dense_network(X)\n",
    "        X = self.out_layer(X)\n",
    "        mu, log_sigmas = torch.split(X, self.k, dim=1) \n",
    "        # cap sigma to value for training stability\n",
    "        #sigmas = torch.clamp(input=sigmas, min=-np.inf, max=2) \n",
    "        return mu, log_sigmas\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_items, dense_layers, batch_norm, activation, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.n_items = n_items\n",
    "        self.additional_layer = len(dense_layers)\n",
    "        if self.additional_layer > 2:\n",
    "            all_layers = [DenseLayers(in_channels=in_channels, out_channels=out_channels, dropout_rate=dropout_rate,\n",
    "                                                          batch_norm=batch_norm, activation=activation) for (in_channels, out_channels) in zip(dense_layers[1:-1], dense_layers[2:])]\n",
    "            \n",
    "            all_layers[0:0] = [DenseLayers(in_channels=dense_layers[0], out_channels=dense_layers[1], batch_norm=batch_norm, dropout_rate=None, activation=activation)]\n",
    "            print(all_layers)\n",
    "            self.dense_network = nn.Sequential(*all_layers)\n",
    "        elif self.additional_layer > 1:\n",
    "            self.dense_network = DenseLayers(in_channels=dense_layers[0], out_channels=dense_layers[1], batch_norm=batch_norm, dropout_rate=None, activation=activation)\n",
    "        else:\n",
    "            pass\n",
    "        self.out_layer = nn.Linear(dense_layers[-1], n_items)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if self.additional_layer > 1:\n",
    "            X = self.dense_network(X)\n",
    "        X = self.out_layer(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseLayers(in_channels, out_channels, batch_norm=False, dropout_rate=None, activation=False):\n",
    "        individual_layers = []\n",
    "            \n",
    "        if not dropout_rate is None:\n",
    "            individual_layers.append(nn.Dropout(p=dropout_rate))\n",
    "            \n",
    "        individual_layers.append(nn.Linear(in_channels, out_channels))\n",
    "\n",
    "        if activation is not None:\n",
    "            individual_layers.append(nn.Tanh())\n",
    "            \n",
    "        if batch_norm is not None:\n",
    "            individual_layers.append(nn.BatchNorm1d(num_features=out_channels))\n",
    "\n",
    "        return nn.Sequential(*individual_layers)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, k, dense_layers, batch_norm, activation, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.k  = k\n",
    "        self.additional_layer = len(dense_layers)\n",
    "        if self.additional_layer > 2:\n",
    "            all_layers = [DenseLayers(in_channels=in_channels, out_channels=out_channels, dropout_rate=dropout_rate,\n",
    "                                                          batch_norm=batch_norm, activation=activation) for (in_channels, out_channels) in zip(dense_layers[1:-1], dense_layers[2:])]\n",
    "            \n",
    "            all_layers[0:0] = [DenseLayers(in_channels=dense_layers[0], out_channels=dense_layers[1], batch_norm=batch_norm, dropout_rate=None, activation=activation)]\n",
    "            self.dense_network = nn.Sequential(*all_layers)\n",
    "        elif self.additional_layer > 1:\n",
    "            self.dense_network = DenseLayers(in_channels=dense_layers[0], out_channels=dense_layers[1], batch_norm=batch_norm, dropout_rate=None, activation=activation)\n",
    "        else:\n",
    "            pass\n",
    "        self.out_layer = nn.Linear(dense_layers[-1], 2 * k)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if self.additional_layer > 1:\n",
    "            X = self.dense_network(X)\n",
    "        X = self.out_layer(X)\n",
    "        mu, log_sigmas = torch.split(X, self.k, dim=1) \n",
    "        # cap sigma to value for training stability\n",
    "        #sigmas = torch.clamp(input=sigmas, min=-np.inf, max=2) \n",
    "        return mu, log_sigmas\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_items, dense_layers, batch_norm, activation, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.n_items = n_items\n",
    "        self.additional_layer = len(dense_layers)\n",
    "        if self.additional_layer > 2:\n",
    "            all_layers = [DenseLayers(in_channels=in_channels, out_channels=out_channels, dropout_rate=dropout_rate,\n",
    "                                                          batch_norm=batch_norm, activation=activation) for (in_channels, out_channels) in zip(dense_layers[1:-1], dense_layers[2:])]\n",
    "            \n",
    "            all_layers[0:0] = [DenseLayers(in_channels=dense_layers[0], out_channels=dense_layers[1], batch_norm=batch_norm, dropout_rate=None, activation=activation)]\n",
    "            print(all_layers)\n",
    "            self.dense_network = nn.Sequential(*all_layers)\n",
    "        elif self.additional_layer > 1:\n",
    "            self.dense_network = DenseLayers(in_channels=dense_layers[0], out_channels=dense_layers[1], batch_norm=batch_norm, dropout_rate=None, activation=activation)\n",
    "        else:\n",
    "            pass\n",
    "        self.out_layer = nn.Linear(dense_layers[-1], n_items)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if self.additional_layer > 1:\n",
    "            X = self.dense_network(X)\n",
    "        X = self.out_layer(X)\n",
    "        return F.log_softmax(X, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mult_VAE(nn.Module):\n",
    "    def __init__(self, device, beta, k, n_items, dense_layers_encoder, dense_layers_decoder, batch_norm_encoder, batch_norm_decoder,\n",
    "                 dropout_rate_encoder, dropout_rate_decoder, optimizer=None, writer=None):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.Encoder = Encoder(k=k, dense_layers=dense_layers_encoder, batch_norm=batch_norm_encoder, activation=True, dropout_rate=dropout_rate_encoder)\n",
    "        self.Decoder = Decoder(n_items=n_items, dense_layers=dense_layers_decoder, batch_norm=batch_norm_decoder, activation=True, dropout_rate=dropout_rate_decoder)\n",
    "        self.device = device\n",
    "        self.beta = beta\n",
    "        self.optimizer = optimizer\n",
    "        self.writer = writer\n",
    "    \n",
    "    def forward(self, X):\n",
    "        mu, log_sigmas = self.Encoder(X)\n",
    "        z = Variable(torch.randn(X.shape[0], self.k,  requires_grad=False)).to(self.device)\n",
    "        log_pi = self.Decoder(z.mul(torch.sqrt(torch.exp(log_sigmas))).add(mu))\n",
    "        return mu, log_sigmas, log_pi\n",
    "    \n",
    "    def loss(self, X, mu, log_sigmas, log_pi): # be careful don't overwrite some pre-defined loss from the super class.\n",
    "        \"\"\"self.writer.add_histogram('mu', mu, global_step=self.epoch)\n",
    "        self.writer.add_histogram('sigmas', sigmas, global_step=self.epoch)\n",
    "        self.writer.add_histogram('pi', pi, global_step=self.epoch)\"\"\"\n",
    "        \n",
    "        #log_likelihood = sparse_dense_mul(X, log_pi).sum(dim=1)\n",
    "        log_likelihood = torch.sparse.sum(sparse_dense_mul_sparse_output(X, log_pi), dim=1).to_dense().to(self.device)\n",
    "        \n",
    "        #log_likelihood = torch.mul(X, pi).sum(dim=1)\n",
    "        #print(f\"log_sigmas.mean(): {log_sigmas.mean()}, log_sigmas.max(): {log_sigmas.max()}, log_sigmas.min(): {log_sigmas.min()}\")\n",
    "        KL = torch.sum(0.5 * (-log_sigmas + torch.exp(log_sigmas) + mu ** 2 - 1), dim=1) * self.beta\n",
    "        \n",
    "        \"\"\"print(f\"KL: {KL.sum()}, log_likelihood: {log_likelihood.sum()}\")\n",
    "        print(f\"mu: {mu.sum()}, log_sigmas: {log_sigmas.sum()}, log_pi: {log_pi.sum()}\")\n",
    "        \n",
    "        self.writer.add_histogram('log_likelihood', log_likelihood, global_step=self.epoch)\n",
    "        self.writer.add_histogram('KL_part1', KL_part1, global_step=self.epoch)\n",
    "        self.writer.add_histogram('KL_part2', KL_part2, global_step=self.epoch)\n",
    "        self.writer.add_histogram('KL_part3', KL_part3, global_step=self.epoch)\n",
    "        self.writer.add_histogram('KL', KL, global_step=self.epoch)\"\"\"\n",
    "        \n",
    "        #KL = 0.5 * (torch.sum(torch.log(sigmas), dim=1) - self.k + torch.sum((1/(sigmas + 1e-10)), dim=1) + torch.sum(torch.mul(sigmas, torch.mul(mu, mu)), dim=1))\n",
    "        #print(f\"log_likleihood: {log_likelihood}, part_1: {KL_part1}, part_2: {KL_part2}, part_3: {KL_part3}, KL: {KL}\")\n",
    "        #print(f\"KL.shape: {KL.shape}, log_likelihood.shape: {log_likelihood.shape}\")\n",
    "        return torch.sum(torch.add(KL, log_likelihood))\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        self.train(True)\n",
    "        self.zero_grad()\n",
    "        mu, log_sigmas, log_pi = self.__call__(X)\n",
    "        loss = self.loss(X=X, mu=mu, log_sigmas=log_sigmas, log_pi=log_pi)\n",
    "        loss.backward()\n",
    "\n",
    "         #print(f\"loss: {loss}\")\n",
    "        \"\"\"for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"{name}: {param.grad.sum()}\")\"\"\"\n",
    "                \n",
    "        # Gradient clipping\n",
    "        clip_grad_norm_(self.parameters(), 20)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def batch_run(self, X, batch_size, verbose=0):\n",
    "        avr_loss = 0.0\n",
    "        \n",
    "        if verbose > 0:\n",
    "            for batch_idx in tqdm(range(0, X.shape[0], batch_size)):\n",
    "                batch_X = make_coo_to_sparse_tensor(X[batch_idx:(batch_idx + batch_size), :].tocoo()).to(self.device)\n",
    "                cur_loss = self.forward_pass(batch_X)\n",
    "                avr_loss += cur_loss\n",
    "        else:\n",
    "            for batch_idx in range(0, X.shape[0], batch_size):\n",
    "                batch_X = make_coo_to_sparse_tensor(X[batch_idx:(batch_idx + batch_size), :].tocoo()).to(self.device)\n",
    "                cur_loss = self.forward_pass(batch_X)\n",
    "                avr_loss += cur_loss\n",
    "        \n",
    "        return avr_loss\n",
    "    \n",
    "    def train_model(self, X_train,  batch_size, epochs, verbose=0):          \n",
    "        if self.optimizer is None:\n",
    "            raise ValueError(\"Please assign first an optimizer to the model with model.optimizer = torch.optim.optimizer\")\n",
    "        \n",
    "        if not self.writer is None:\n",
    "            # Log the model graph with a small example.\n",
    "            self.writer.add_graph(self, make_coo_to_sparse_tensor(X_train[:min(X_train.shape[0], 10)].tocoo()).to(self.device))\n",
    "\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            self.epoch=epoch # RM LATER!!\n",
    "            avr_loss_train  = self.batch_run(X=X_train, batch_size=batch_size, verbose=0)\n",
    "\n",
    "            if not self.writer is None:\n",
    "                self.writer.add_scalar('loss_train', avr_loss_train, global_step=epoch)\n",
    "                \"\"\"for name, params in self.named_parameters():\n",
    "                    self.writer.add_histogram(name, params, global_step=epoch)\"\"\"\n",
    "\n",
    "            if verbose > 0:\n",
    "                print(f\" test Performance in epoch {epoch} was {avr_loss_train}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  (0): Linear(in_features=10, out_features=100, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "), Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (2): Tanh()\n",
      "  (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "), Sequential(\n",
      "  (0): Dropout(p=0.5, inplace=False)\n",
      "  (1): Linear(in_features=100, out_features=1000, bias=True)\n",
      "  (2): Tanh()\n",
      "  (3): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1573049306803/work/torch/csrc/jit/pybind_utils.h:357: UserWarning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted.\n",
      "/opt/conda/conda-bld/pytorch_1573049306803/work/torch/csrc/jit/pybind_utils.h:586: UserWarning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted.\n",
      "/opt/conda/conda-bld/pytorch_1573049306803/work/torch/csrc/jit/pybind_utils.h:357: UserWarning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted.\n",
      "/opt/conda/conda-bld/pytorch_1573049306803/work/torch/csrc/jit/pybind_utils.h:586: UserWarning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted.\n",
      "/opt/conda/conda-bld/pytorch_1573049306803/work/torch/csrc/jit/pybind_utils.h:357: UserWarning: Using sparse tensors in TorchScript is experimental. Many optimization pathways have not been thoroughly tested with sparse tensors. Please include the fact that the network is running sparse tensors in any bug reports submitted.\n",
      "/home/titoeb/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py:1007: TracerWarning: Trace had nondeterministic nodes. Did you forget call .eval() on your model? Nodes:\n",
      "\t%140 : Float(10, 10) = aten::randn(%135, %136, %137, %138, %139), scope: Mult_VAE # <ipython-input-7-9d0e8a998e3a>:15:0\n",
      "This may cause errors in trace checking. To disable trace checking, pass check_trace=False to torch.jit.trace()\n",
      "  check_tolerance, _force_outplace, True, _module_class)\n",
      "/home/titoeb/anaconda3/lib/python3.7/site-packages/torch/jit/__init__.py:1007: TracerWarning: Output nr 3. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Not within tolerance rtol=1e-05 atol=1e-05 at input[4, 19463] (-9.855230331420898 vs. -10.529387474060059) and 262010 other locations (99.00%)\n",
      "  check_tolerance, _force_outplace, True, _module_class)\n",
      "  1%|          | 1/100 [00:49<1:21:46, 49.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 0 was -673633600.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [01:38<1:20:43, 49.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 1 was -2401815040.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [02:27<1:19:43, 49.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 2 was -4766439936.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [03:16<1:18:49, 49.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 3 was -7137920000.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [04:07<1:18:43, 49.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 4 was -9834000384.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [04:56<1:17:38, 49.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 5 was -12470851584.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [05:45<1:16:37, 49.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 6 was -14209414144.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [06:35<1:15:38, 49.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 7 was -16338203648.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [07:24<1:14:44, 49.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 8 was -19025678336.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [08:13<1:13:53, 49.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 9 was -22465705984.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [09:02<1:13:05, 49.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 10 was -26476201984.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [09:51<1:12:05, 49.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 11 was -30945871872.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [10:40<1:11:06, 49.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 12 was -35843248128.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [11:29<1:10:20, 49.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 13 was -40803627008.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [12:18<1:09:36, 49.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 14 was -46130581504.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [13:08<1:08:48, 49.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 15 was -52454580224.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [13:57<1:08:00, 49.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 16 was -59365064704.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [14:46<1:07:11, 49.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 17 was -67157442560.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [15:35<1:06:23, 49.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 18 was -75120451584.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [16:24<1:05:34, 49.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 19 was -83316875264.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [17:14<1:04:53, 49.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 20 was -92633710592.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [18:03<1:04:06, 49.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 21 was -103016177664.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [18:52<1:03:15, 49.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 22 was -113826299904.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [19:43<1:02:59, 49.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 23 was -125723746304.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [20:33<1:02:11, 49.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 24 was -138027728896.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [21:23<1:01:35, 49.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 25 was -151447060480.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [22:13<1:00:42, 49.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 26 was -165187698688.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [23:03<59:50, 49.87s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 27 was -180930052096.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [23:53<59:05, 49.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 28 was -197942001664.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [24:43<58:20, 50.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test Performance in epoch 29 was -215263019008.0.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-445d156672af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-9d0e8a998e3a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, X_train, batch_size, epochs, verbose)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;31m# RM LATER!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mavr_loss_train\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9d0e8a998e3a>\u001b[0m in \u001b[0;36mbatch_run\u001b[0;34m(self, X, batch_size, verbose)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0mbatch_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_coo_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0mcur_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mavr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcur_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = Mult_VAE(device=device, k = 10, n_items = train_mat.shape[1], dense_layers_encoder=[train_mat.shape[1], 1000,100, 100], dense_layers_decoder=[10, 100, 100, 1000], batch_norm_encoder=True,\n",
    "                batch_norm_decoder=True, dropout_rate_decoder=0.5, dropout_rate_encoder=0.5, beta=0.2).to(device)\n",
    "\n",
    "time_stamp = ''.join(str(datetime.now()).split('.')[:-1]).replace(' ', '_').replace(':', '_').replace('-', '_')\n",
    "test.writer = SummaryWriter('./logs/' + time_stamp)\n",
    "\n",
    "test.optimizer = optim.Adam(test.parameters(), lr=1e-3)\n",
    "test.train_model(X_train=train_mat, batch_size=500, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.sparse.FloatTensor(\n",
    "    indices=torch.LongTensor([[0, 1, 1], [2, 0, 2]]),\n",
    "    values=torch.FloatTensor([3, 4, 5]),\n",
    "    size=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 3.],\n",
       "        [4., 0., 5.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 9.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.to_dense().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 9.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sparse.sum(x, dim=1).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'todense'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-56723253c355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'todense'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_sum_dim_1(sparse_tensor):\n",
    "    x = torch.sparse.FloatTensor(\n",
    "            indices=torch.stack([\n",
    "                sparse_tensor._indices()[0],\n",
    "                torch.LongTensor(1).zero_().expand_as(sparse_tensor._indices()[0]),\n",
    "            ]),\n",
    "            values=sparse_tensor._values(),\n",
    "            size=[sparse_tensor.shape[0], 1])\n",
    "\n",
    "    x.coalesce()\n",
    "    return x.to_dense().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 9.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_sum_dim_1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0064,     nan, -0.0148,  ...,     nan,     nan,     nan],\n",
       "        [ 0.0147,     nan, -0.0120,  ...,     nan,     nan,     nan],\n",
       "        [ 0.0124,     nan,  0.0091,  ...,     nan,     nan,     nan],\n",
       "        ...,\n",
       "        [-0.0115,     nan, -0.0006,  ...,     nan,     nan,     nan],\n",
       "        [ 0.0030,     nan, -0.0130,  ...,     nan,     nan,     nan],\n",
       "        [-0.0078,     nan,  0.0139,  ...,     nan,     nan,     nan]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(test.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder.dense_network.0.0.weight: nan\n",
      "Encoder.dense_network.0.0.bias: nan\n",
      "Encoder.dense_network.1.0.weight: nan\n",
      "Encoder.dense_network.1.0.bias: nan\n",
      "Encoder.dense_network.2.0.weight: nan\n",
      "Encoder.dense_network.2.0.bias: nan\n",
      "Encoder.out_layer.weight: nan\n",
      "Encoder.out_layer.bias: nan\n",
      "Decoder.dense_network.0.0.weight: nan\n",
      "Decoder.dense_network.0.0.bias: nan\n",
      "Decoder.dense_network.1.0.weight: nan\n",
      "Decoder.dense_network.1.0.bias: nan\n",
      "Decoder.dense_network.2.0.weight: nan\n",
      "Decoder.dense_network.2.0.bias: nan\n",
      "Decoder.out_layer.weight: nan\n",
      "Decoder.out_layer.bias: nan\n"
     ]
    }
   ],
   "source": [
    "for name, param in test.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.grad.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., nan, 0.,  ..., nan, nan, nan],\n",
       "        [0., nan, 0.,  ..., nan, nan, nan],\n",
       "        [0., nan, 0.,  ..., nan, nan, nan],\n",
       "        ...,\n",
       "        [0., nan, 0.,  ..., nan, nan, nan],\n",
       "        [0., nan, 0.,  ..., nan, nan, nan],\n",
       "        [0., nan, 0.,  ..., nan, nan, nan]], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Encoder.dense_network.0.0.weight'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Encoder.dense_network.0.0.weight', Parameter containing:\n",
       "  tensor([[-0.0074,     nan, -0.0067,  ...,     nan,     nan,     nan],\n",
       "          [ 0.0101,     nan,  0.0015,  ...,     nan,     nan,     nan],\n",
       "          [ 0.0149,     nan,  0.0033,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [ 0.0157,     nan, -0.0033,  ...,     nan,     nan,     nan],\n",
       "          [ 0.0045,     nan, -0.0097,  ...,     nan,     nan,     nan],\n",
       "          [-0.0010,     nan,  0.0100,  ...,     nan,     nan,     nan]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('Encoder.dense_network.0.0.bias', Parameter containing:\n",
       "  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('Encoder.dense_network.1.0.weight', Parameter containing:\n",
       "  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "         requires_grad=True)),\n",
       " ('Encoder.dense_network.1.0.bias', Parameter containing:\n",
       "  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan], device='cuda:0', requires_grad=True)),\n",
       " ('Encoder.dense_network.2.0.weight', Parameter containing:\n",
       "  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "         requires_grad=True)),\n",
       " ('Encoder.dense_network.2.0.bias', Parameter containing:\n",
       "  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan], device='cuda:0', requires_grad=True)),\n",
       " ('Encoder.out_layer.weight', Parameter containing:\n",
       "  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "         requires_grad=True)),\n",
       " ('Encoder.out_layer.bias', Parameter containing:\n",
       "  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('Decoder.dense_network.0.0.weight', Parameter containing:\n",
       "  tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "          [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
       "         requires_grad=True)),\n",
       " ('Decoder.dense_network.0.0.bias', Parameter containing:\n",
       "  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan], device='cuda:0', requires_grad=True)),\n",
       " ('Decoder.dense_network.1.0.weight', Parameter containing:\n",
       "  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "         requires_grad=True)),\n",
       " ('Decoder.dense_network.1.0.bias', Parameter containing:\n",
       "  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan], device='cuda:0', requires_grad=True)),\n",
       " ('Decoder.dense_network.2.0.weight', Parameter containing:\n",
       "  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "         requires_grad=True)),\n",
       " ('Decoder.dense_network.2.0.bias', Parameter containing:\n",
       "  tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
       "          nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('Decoder.out_layer.weight', Parameter containing:\n",
       "  tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0',\n",
       "         requires_grad=True)),\n",
       " ('Decoder.out_layer.bias', Parameter containing:\n",
       "  tensor([nan, nan, nan,  ..., nan, nan, nan], device='cuda:0',\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test.named_parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
