{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitanaconda3virtualenv46adda40fba449109a0adfc6bea6de6c",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: The RecModel package for implicit feedback\n",
    "\n",
    "## Get the Data\n",
    "In the following we test the SLIM model based on the Netflix dataset. As all models in the RecModel package use csr matrix as input, we need to download the Netflix dataset and convert it to csr.\n",
    "\n",
    "Fortunately, I created a small repo that does exatly that: [Data](https://github.com/titoeb/ImplicitFeedback).\n",
    "\n",
    "Do the following steps to create the Netflix dataset:\n",
    "\n",
    "* Clone the Repository\n",
    "    ```\n",
    "    git clone https://github.com/titoeb/ImplicitFeedback\n",
    "    ```\n",
    "\n",
    "* Change into the repository:\n",
    "    ```\n",
    "    cd ImplicitFeedback\n",
    "    ```\n",
    "* Allow execution of the Bash Script:\n",
    "    ```\n",
    "    chmod +x Create_Data\n",
    "    ```\n",
    "\n",
    "* Execute the Bash Script. This may take some time as the Netflix, ML20 and Million Song Dataset is download.\n",
    "    ```\n",
    "    ./Create_Data\n",
    "    ```\n",
    "\n",
    "* Now the data is downloaded. To create the CSR matrix run the Netflix.py script.\n",
    "    ```\n",
    "    python Netflix.py\n",
    "    ```\n",
    "* Now you simply copy the Netflix.npz file into the data folder in your directory of the RecModel Package. You can delete the Implicit Feedback Data folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import the data, fit the SLIM model and analyze the results we need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import numpy as np\n",
    "import RecModel\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recmodel package implements the following models:\n",
    "\n",
    "* [Neighbor](https://dl.acm.org/doi/10.1145/371920.372071): RecModel.Neighborhood\n",
    "\n",
    "* [SLIM](https://dl.acm.org/doi/10.1109/ICDM.2011.134): RecModel.SLIM\n",
    "\n",
    "* [VAE](https://dl.acm.org/doi/abs/10.1145/3178876.3186150): RecModel.VAE\n",
    "\n",
    "* [EASE](https://dl.acm.org/doi/abs/10.1145/3308558.3313710): RecModel.EASE\n",
    "\n",
    "* [WMF](https://dl.acm.org/doi/10.1109/ICDM.2008.22): RecModel.WMF\n",
    "\n",
    "* [RecWalk](https://dl.acm.org/doi/abs/10.1145/3289600.3291016): RecModel.RecWalk\n",
    "\n",
    "All these models have different hyper parameters, for mor details look in the Documentation of the individual models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we fit any models we need to load the netflix data. To speed up computation, we only use the first 10000 users and the first 2500 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_data = scipy.sparse.load_npz('data/Netflix.npz')[:10000, :2500]\n",
    "\n",
    "num_users, num_items = netflix_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And split it into train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = RecModel.train_test_split_sparse_mat(netflix_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, first we are going to fit a baseline model, that predicts random items to users. Afterwards, we are going to fit the SLIM model, a regularized, linear model model.\n",
    "\n",
    "Let's start with the Baseline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Baseline only samples random items for every user. Therefore it does not need be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = RecModel.NaiveBaseline(num_items=num_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the recall@4, recall@10, recall@20 and recall@50 performance based on 1000 random items for each user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model_performance = naive_model.eval_topn(test_mat=test_data, topn = np.array([4, 10, 20, 50]), rand_sampled=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'Recall@4': 0.0055808444, 'Recall@10': 0.014061233, 'Recall@20': 0.027124774, 'Recall@50': 0.06940201}\n"
    }
   ],
   "source": [
    "print(naive_model_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random sampling items of items only gets us a recall@50 of about 5 percent (as one would expect when sampling 50 out of 1000 items). Let's use the SLIM model to improve the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_model = RecModel.SLIM(num_items=num_items, num_users=num_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SLIM model is an actual, model-based collaborative filtering model. Therefore it needst to be trained. Unfortunately, the SLIM model is expensive to train and would need about an hour of training time based on the full netlix dataset. To get more information about the status during training, set verbose to True!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_model.train(X=train_data.astype(np.float64), alpha=4.427181, l1_ratio=0.318495, max_iter=27, tolerance=0.006841, cores=8, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we trained the model, we can check its performance on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_model_performance = slim_model.eval_topn(test_mat=test_data.astype(np.float64), topn = np.array([4, 10, 20, 50]), rand_sampled=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'Recall@4': 0.17652927, 'Recall@10': 0.3280227, 'Recall@20': 0.48915008, 'Recall@50': 0.7147534}\n"
    }
   ],
   "source": [
    "print(slim_model_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Slim model already gets a recall@4 performance of about 17.6 percent and a recall@50 performance of 71.4 percent. Finally some solid numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}